{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BReldAj3G81n"
   },
   "source": [
    "# ResNeXt [7 points]\n",
    "Based on your ResNet implementation in Part I, extend it to ResNeXT. It is expected that your accuracy is higher than ResNet. Compare the results with your VGG and ResNet implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERS4g8fhG81p"
   },
   "source": [
    "## Step 1: Implement the ResNeXT architecture\n",
    "Pay close attention to the grouped convolutions and cardinality parameter. Using inbuild ResNeXt model wonâ€™t be considered for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YUSitlbRMwm1"
   },
   "outputs": [],
   "source": [
    "!unzip -q cnn_dataset.zip -d /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzPLWKwzG81q"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as function\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Aw6lWdAMp1s",
    "outputId": "aad183e0-7d3f-4cfd-80d2-d7d91bc84df5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Category  Image Count\n",
      "0      dogs        10000\n",
      "1      food        10000\n",
      "2  vehicles        10000\n",
      "classes: ['dogs', 'food', 'vehicles']\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"cnn_dataset\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root = dataset_path, transform = transform)\n",
    "num_classes = len(dataset.classes)\n",
    "num_images = len(dataset)\n",
    "class_names = dataset.classes\n",
    "class_distribution = {cls: 0 for cls in class_names}\n",
    "class_sums = {cls: torch.zeros((3, 96, 96)) for cls in class_names}\n",
    "\n",
    "\n",
    "for img, label in dataset:\n",
    "    class_distribution[class_names[label]] += 1\n",
    "    class_sums[class_names[label]] += img\n",
    "\n",
    "dataset_stats = pd.DataFrame({\n",
    "    \"Category\": list(class_distribution.keys()),\n",
    "    \"Image Count\": list(class_distribution.values())\n",
    "})\n",
    "\n",
    "print(dataset_stats)\n",
    "\n",
    "print(\"classes:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dWXGAvNpNCzw"
   },
   "outputs": [],
   "source": [
    "def create_one_hot_dataset(original_dataset, num_classes):\n",
    "    images = []\n",
    "    one_hot_labels = []\n",
    "\n",
    "    for img, label in original_dataset:\n",
    "        images.append(img)\n",
    "        label_long = torch.tensor(label, dtype=torch.long)\n",
    "        one_hot = function.one_hot(label_long, num_classes=num_classes)\n",
    "        one_hot_labels.append(one_hot)\n",
    "\n",
    "    images_tensor = torch.stack(images)\n",
    "    labels_tensor = torch.stack(one_hot_labels)\n",
    "\n",
    "    return TensorDataset(images_tensor, labels_tensor)\n",
    "\n",
    "\n",
    "dataset = create_one_hot_dataset(dataset, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kbmNDKRuNKJy",
    "outputId": "fc1b9261-99c3-423b-9f9b-696e33ffca70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 21000\n",
      "Validation set size: 4500\n",
      "Test set size: 4500\n"
     ]
    }
   ],
   "source": [
    "indices = list(range(len(dataset)))\n",
    "train_indices, temp_indices = train_test_split(indices, test_size = 0.30, random_state=42)\n",
    "val_indices, test_indices = train_test_split(temp_indices, test_size = 0.50, random_state=42)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64,shuffle=False, num_workers=4)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-VC06kFxN7jv"
   },
   "outputs": [],
   "source": [
    "class ResNeXtBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, cardinality=32, base_width=4, dropout_val=0.0):\n",
    "        super(ResNeXtBlock, self).__init__()\n",
    "        D = cardinality * base_width\n",
    "        self.conv1 = nn.Conv2d(in_channels, D, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(D)\n",
    "        self.conv2 = nn.Conv2d(D, D, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(D)\n",
    "        self.conv3 = nn.Conv2d(D, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.dropout = nn.Dropout(p=dropout_val) if dropout_val > 0 else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        if self.dropout is not None:\n",
    "            out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNeXt(nn.Module):\n",
    "    def __init__(self, num_classes, cardinality=32, base_width=4, dropout_val=0.0):\n",
    "        super(ResNeXt, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(64, 3, stride=1, cardinality=cardinality, base_width=base_width, dropout_val=dropout_val)\n",
    "        self.layer2 = self._make_layer(128, 4, stride=2, cardinality=cardinality, base_width=base_width, dropout_val=dropout_val)\n",
    "        self.layer3 = self._make_layer(256, 6, stride=2, cardinality=cardinality, base_width=base_width, dropout_val=dropout_val)\n",
    "        self.layer4 = self._make_layer(512, 3, stride=2, cardinality=cardinality, base_width=base_width, dropout_val=dropout_val)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(p=dropout_val) if dropout_val > 0 else None\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, out_channels, blocks, stride, cardinality, base_width, dropout_val):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(ResNeXtBlock(self.in_channels, out_channels, stride, downsample, cardinality, base_width, dropout_val))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResNeXtBlock(self.in_channels, out_channels, 1, None, cardinality, base_width, dropout_val))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gkUuzzqAQ-6s",
    "outputId": "ddea71fd-7ec2-4e57-c0a2-3ed80178b33d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5          [-1, 128, 56, 56]           8,192\n",
      "       BatchNorm2d-6          [-1, 128, 56, 56]             256\n",
      "              ReLU-7          [-1, 128, 56, 56]               0\n",
      "            Conv2d-8          [-1, 128, 56, 56]           4,608\n",
      "       BatchNorm2d-9          [-1, 128, 56, 56]             256\n",
      "             ReLU-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11           [-1, 64, 56, 56]           8,192\n",
      "      BatchNorm2d-12           [-1, 64, 56, 56]             128\n",
      "             ReLU-13           [-1, 64, 56, 56]               0\n",
      "     ResNeXtBlock-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15          [-1, 128, 56, 56]           8,192\n",
      "      BatchNorm2d-16          [-1, 128, 56, 56]             256\n",
      "             ReLU-17          [-1, 128, 56, 56]               0\n",
      "           Conv2d-18          [-1, 128, 56, 56]           4,608\n",
      "      BatchNorm2d-19          [-1, 128, 56, 56]             256\n",
      "             ReLU-20          [-1, 128, 56, 56]               0\n",
      "           Conv2d-21           [-1, 64, 56, 56]           8,192\n",
      "      BatchNorm2d-22           [-1, 64, 56, 56]             128\n",
      "             ReLU-23           [-1, 64, 56, 56]               0\n",
      "     ResNeXtBlock-24           [-1, 64, 56, 56]               0\n",
      "           Conv2d-25          [-1, 128, 56, 56]           8,192\n",
      "      BatchNorm2d-26          [-1, 128, 56, 56]             256\n",
      "             ReLU-27          [-1, 128, 56, 56]               0\n",
      "           Conv2d-28          [-1, 128, 56, 56]           4,608\n",
      "      BatchNorm2d-29          [-1, 128, 56, 56]             256\n",
      "             ReLU-30          [-1, 128, 56, 56]               0\n",
      "           Conv2d-31           [-1, 64, 56, 56]           8,192\n",
      "      BatchNorm2d-32           [-1, 64, 56, 56]             128\n",
      "             ReLU-33           [-1, 64, 56, 56]               0\n",
      "     ResNeXtBlock-34           [-1, 64, 56, 56]               0\n",
      "           Conv2d-35          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-36          [-1, 128, 28, 28]             256\n",
      "           Conv2d-37          [-1, 128, 56, 56]           8,192\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]           4,608\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 128, 28, 28]          16,384\n",
      "      BatchNorm2d-44          [-1, 128, 28, 28]             256\n",
      "             ReLU-45          [-1, 128, 28, 28]               0\n",
      "     ResNeXtBlock-46          [-1, 128, 28, 28]               0\n",
      "           Conv2d-47          [-1, 128, 28, 28]          16,384\n",
      "      BatchNorm2d-48          [-1, 128, 28, 28]             256\n",
      "             ReLU-49          [-1, 128, 28, 28]               0\n",
      "           Conv2d-50          [-1, 128, 28, 28]           4,608\n",
      "      BatchNorm2d-51          [-1, 128, 28, 28]             256\n",
      "             ReLU-52          [-1, 128, 28, 28]               0\n",
      "           Conv2d-53          [-1, 128, 28, 28]          16,384\n",
      "      BatchNorm2d-54          [-1, 128, 28, 28]             256\n",
      "             ReLU-55          [-1, 128, 28, 28]               0\n",
      "     ResNeXtBlock-56          [-1, 128, 28, 28]               0\n",
      "           Conv2d-57          [-1, 128, 28, 28]          16,384\n",
      "      BatchNorm2d-58          [-1, 128, 28, 28]             256\n",
      "             ReLU-59          [-1, 128, 28, 28]               0\n",
      "           Conv2d-60          [-1, 128, 28, 28]           4,608\n",
      "      BatchNorm2d-61          [-1, 128, 28, 28]             256\n",
      "             ReLU-62          [-1, 128, 28, 28]               0\n",
      "           Conv2d-63          [-1, 128, 28, 28]          16,384\n",
      "      BatchNorm2d-64          [-1, 128, 28, 28]             256\n",
      "             ReLU-65          [-1, 128, 28, 28]               0\n",
      "     ResNeXtBlock-66          [-1, 128, 28, 28]               0\n",
      "           Conv2d-67          [-1, 128, 28, 28]          16,384\n",
      "      BatchNorm2d-68          [-1, 128, 28, 28]             256\n",
      "             ReLU-69          [-1, 128, 28, 28]               0\n",
      "           Conv2d-70          [-1, 128, 28, 28]           4,608\n",
      "      BatchNorm2d-71          [-1, 128, 28, 28]             256\n",
      "             ReLU-72          [-1, 128, 28, 28]               0\n",
      "           Conv2d-73          [-1, 128, 28, 28]          16,384\n",
      "      BatchNorm2d-74          [-1, 128, 28, 28]             256\n",
      "             ReLU-75          [-1, 128, 28, 28]               0\n",
      "     ResNeXtBlock-76          [-1, 128, 28, 28]               0\n",
      "           Conv2d-77          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-78          [-1, 256, 14, 14]             512\n",
      "           Conv2d-79          [-1, 128, 28, 28]          16,384\n",
      "      BatchNorm2d-80          [-1, 128, 28, 28]             256\n",
      "             ReLU-81          [-1, 128, 28, 28]               0\n",
      "           Conv2d-82          [-1, 128, 14, 14]           4,608\n",
      "      BatchNorm2d-83          [-1, 128, 14, 14]             256\n",
      "             ReLU-84          [-1, 128, 14, 14]               0\n",
      "           Conv2d-85          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-86          [-1, 256, 14, 14]             512\n",
      "             ReLU-87          [-1, 256, 14, 14]               0\n",
      "     ResNeXtBlock-88          [-1, 256, 14, 14]               0\n",
      "           Conv2d-89          [-1, 128, 14, 14]          32,768\n",
      "      BatchNorm2d-90          [-1, 128, 14, 14]             256\n",
      "             ReLU-91          [-1, 128, 14, 14]               0\n",
      "           Conv2d-92          [-1, 128, 14, 14]           4,608\n",
      "      BatchNorm2d-93          [-1, 128, 14, 14]             256\n",
      "             ReLU-94          [-1, 128, 14, 14]               0\n",
      "           Conv2d-95          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-96          [-1, 256, 14, 14]             512\n",
      "             ReLU-97          [-1, 256, 14, 14]               0\n",
      "     ResNeXtBlock-98          [-1, 256, 14, 14]               0\n",
      "           Conv2d-99          [-1, 128, 14, 14]          32,768\n",
      "     BatchNorm2d-100          [-1, 128, 14, 14]             256\n",
      "            ReLU-101          [-1, 128, 14, 14]               0\n",
      "          Conv2d-102          [-1, 128, 14, 14]           4,608\n",
      "     BatchNorm2d-103          [-1, 128, 14, 14]             256\n",
      "            ReLU-104          [-1, 128, 14, 14]               0\n",
      "          Conv2d-105          [-1, 256, 14, 14]          32,768\n",
      "     BatchNorm2d-106          [-1, 256, 14, 14]             512\n",
      "            ReLU-107          [-1, 256, 14, 14]               0\n",
      "    ResNeXtBlock-108          [-1, 256, 14, 14]               0\n",
      "          Conv2d-109          [-1, 128, 14, 14]          32,768\n",
      "     BatchNorm2d-110          [-1, 128, 14, 14]             256\n",
      "            ReLU-111          [-1, 128, 14, 14]               0\n",
      "          Conv2d-112          [-1, 128, 14, 14]           4,608\n",
      "     BatchNorm2d-113          [-1, 128, 14, 14]             256\n",
      "            ReLU-114          [-1, 128, 14, 14]               0\n",
      "          Conv2d-115          [-1, 256, 14, 14]          32,768\n",
      "     BatchNorm2d-116          [-1, 256, 14, 14]             512\n",
      "            ReLU-117          [-1, 256, 14, 14]               0\n",
      "    ResNeXtBlock-118          [-1, 256, 14, 14]               0\n",
      "          Conv2d-119          [-1, 128, 14, 14]          32,768\n",
      "     BatchNorm2d-120          [-1, 128, 14, 14]             256\n",
      "            ReLU-121          [-1, 128, 14, 14]               0\n",
      "          Conv2d-122          [-1, 128, 14, 14]           4,608\n",
      "     BatchNorm2d-123          [-1, 128, 14, 14]             256\n",
      "            ReLU-124          [-1, 128, 14, 14]               0\n",
      "          Conv2d-125          [-1, 256, 14, 14]          32,768\n",
      "     BatchNorm2d-126          [-1, 256, 14, 14]             512\n",
      "            ReLU-127          [-1, 256, 14, 14]               0\n",
      "    ResNeXtBlock-128          [-1, 256, 14, 14]               0\n",
      "          Conv2d-129          [-1, 128, 14, 14]          32,768\n",
      "     BatchNorm2d-130          [-1, 128, 14, 14]             256\n",
      "            ReLU-131          [-1, 128, 14, 14]               0\n",
      "          Conv2d-132          [-1, 128, 14, 14]           4,608\n",
      "     BatchNorm2d-133          [-1, 128, 14, 14]             256\n",
      "            ReLU-134          [-1, 128, 14, 14]               0\n",
      "          Conv2d-135          [-1, 256, 14, 14]          32,768\n",
      "     BatchNorm2d-136          [-1, 256, 14, 14]             512\n",
      "            ReLU-137          [-1, 256, 14, 14]               0\n",
      "    ResNeXtBlock-138          [-1, 256, 14, 14]               0\n",
      "          Conv2d-139            [-1, 512, 7, 7]         131,072\n",
      "     BatchNorm2d-140            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-141          [-1, 128, 14, 14]          32,768\n",
      "     BatchNorm2d-142          [-1, 128, 14, 14]             256\n",
      "            ReLU-143          [-1, 128, 14, 14]               0\n",
      "          Conv2d-144            [-1, 128, 7, 7]           4,608\n",
      "     BatchNorm2d-145            [-1, 128, 7, 7]             256\n",
      "            ReLU-146            [-1, 128, 7, 7]               0\n",
      "          Conv2d-147            [-1, 512, 7, 7]          65,536\n",
      "     BatchNorm2d-148            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-149            [-1, 512, 7, 7]               0\n",
      "    ResNeXtBlock-150            [-1, 512, 7, 7]               0\n",
      "          Conv2d-151            [-1, 128, 7, 7]          65,536\n",
      "     BatchNorm2d-152            [-1, 128, 7, 7]             256\n",
      "            ReLU-153            [-1, 128, 7, 7]               0\n",
      "          Conv2d-154            [-1, 128, 7, 7]           4,608\n",
      "     BatchNorm2d-155            [-1, 128, 7, 7]             256\n",
      "            ReLU-156            [-1, 128, 7, 7]               0\n",
      "          Conv2d-157            [-1, 512, 7, 7]          65,536\n",
      "     BatchNorm2d-158            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-159            [-1, 512, 7, 7]               0\n",
      "    ResNeXtBlock-160            [-1, 512, 7, 7]               0\n",
      "          Conv2d-161            [-1, 128, 7, 7]          65,536\n",
      "     BatchNorm2d-162            [-1, 128, 7, 7]             256\n",
      "            ReLU-163            [-1, 128, 7, 7]               0\n",
      "          Conv2d-164            [-1, 128, 7, 7]           4,608\n",
      "     BatchNorm2d-165            [-1, 128, 7, 7]             256\n",
      "            ReLU-166            [-1, 128, 7, 7]               0\n",
      "          Conv2d-167            [-1, 512, 7, 7]          65,536\n",
      "     BatchNorm2d-168            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-169            [-1, 512, 7, 7]               0\n",
      "    ResNeXtBlock-170            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-171            [-1, 512, 1, 1]               0\n",
      "          Linear-172                    [-1, 3]           1,539\n",
      "================================================================\n",
      "Total params: 1,183,683\n",
      "Trainable params: 1,183,683\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 155.00\n",
      "Params size (MB): 4.52\n",
      "Estimated Total Size (MB): 160.08\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = ResNeXt(num_classes=num_classes, cardinality=32, base_width=4, dropout_val=0.0).to(device)\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SNvbQxxDYyXB",
    "outputId": "904aef45-f7cd-4d0e-c1ae-6d7cf324c1bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "           Dropout-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "    ResidualBlock-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-12           [-1, 64, 56, 56]             128\n",
      "          Dropout-13           [-1, 64, 56, 56]               0\n",
      "           Conv2d-14           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 56, 56]             128\n",
      "    ResidualBlock-16           [-1, 64, 56, 56]               0\n",
      "           Conv2d-17          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-18          [-1, 128, 28, 28]             256\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "          Dropout-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "    ResidualBlock-24          [-1, 128, 28, 28]               0\n",
      "           Conv2d-25          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-26          [-1, 128, 28, 28]             256\n",
      "          Dropout-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "    ResidualBlock-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-32          [-1, 256, 14, 14]             512\n",
      "           Conv2d-33          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-34          [-1, 256, 14, 14]             512\n",
      "          Dropout-35          [-1, 256, 14, 14]               0\n",
      "           Conv2d-36          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-37          [-1, 256, 14, 14]             512\n",
      "    ResidualBlock-38          [-1, 256, 14, 14]               0\n",
      "           Conv2d-39          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-40          [-1, 256, 14, 14]             512\n",
      "          Dropout-41          [-1, 256, 14, 14]               0\n",
      "           Conv2d-42          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-43          [-1, 256, 14, 14]             512\n",
      "    ResidualBlock-44          [-1, 256, 14, 14]               0\n",
      "           Conv2d-45            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-46            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-47            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-48            [-1, 512, 7, 7]           1,024\n",
      "          Dropout-49            [-1, 512, 7, 7]               0\n",
      "           Conv2d-50            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-51            [-1, 512, 7, 7]           1,024\n",
      "    ResidualBlock-52            [-1, 512, 7, 7]               0\n",
      "           Conv2d-53            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-54            [-1, 512, 7, 7]           1,024\n",
      "          Dropout-55            [-1, 512, 7, 7]               0\n",
      "           Conv2d-56            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "    ResidualBlock-58            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-59            [-1, 512, 1, 1]               0\n",
      "          Dropout-60                  [-1, 512]               0\n",
      "           Linear-61                    [-1, 3]           1,539\n",
      "================================================================\n",
      "Total params: 11,178,051\n",
      "Trainable params: 11,178,051\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 57.05\n",
      "Params size (MB): 42.64\n",
      "Estimated Total Size (MB): 100.26\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None, dropout_val = 0.5):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride=stride, padding = 1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.dropout = nn.Dropout(p = dropout_val)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = function.relu(out, inplace = True)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        out = self.bn2(out)\n",
    "        out += identity\n",
    "        out = function.relu(out, inplace = True)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_size = 0.5):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.dropout_size = dropout_size\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride = 2, padding = 1)\n",
    "\n",
    "        self.layer1 = self.make_layer(64, 2, stride = 1, dropout_size = self.dropout_size)\n",
    "        self.layer2 = self.make_layer(128, 2, stride = 2, dropout_size = self.dropout_size)\n",
    "        self.layer3 = self.make_layer(256, 2, stride = 2, dropout_size = self.dropout_size)\n",
    "        self.layer4 = self.make_layer(512, 2, stride = 2, dropout_size = self.dropout_size)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(p = dropout_size)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def make_layer(self, out_channels, blocks, stride, dropout_size):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(nn.Conv2d(self.in_channels, out_channels, kernel_size = 1, stride = stride, bias = False), nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(self.in_channels, out_channels, stride, downsample, dropout_val = dropout_size))\n",
    "\n",
    "        self.in_channels = out_channels\n",
    "\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "model = ResNet18(num_classes=num_classes).to(device)\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G29Im36OG81r"
   },
   "source": [
    "## Step 2: Train and evaluate your ResNeXt model\n",
    "Train and evaluate your ResNeXt model on the same dataset used in Part I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "t5R1mRwWTY59"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.argmax(dim=1).long()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.argmax(dim = 1).long()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optim, optim_str, scheduler = None, num_epochs = 10, patience = 3, init_type = 'he', batch_size = 32):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_weights = None\n",
    "    epochs_no_improve = 0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [],'val_acc': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optim)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1} of {num_epochs}]: \" f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_weights = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    if best_model_weights is not None:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3MNNfWgMG81r",
    "outputId": "ef37bdd2-8f6d-44c6-bdf7-3d2d6064e6cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNeXT = SGD, batch_size = 64 ---\n",
      "Epoch [1 of 20]: Train Loss: 0.9341, Train Acc: 0.6817 | Val Loss: 0.5709, Val Acc: 0.7858\n",
      "Epoch [2 of 20]: Train Loss: 0.4752, Train Acc: 0.8185 | Val Loss: 0.4709, Val Acc: 0.8244\n",
      "Epoch [3 of 20]: Train Loss: 0.4033, Train Acc: 0.8473 | Val Loss: 0.3870, Val Acc: 0.8433\n",
      "Epoch [4 of 20]: Train Loss: 0.3664, Train Acc: 0.8625 | Val Loss: 0.4725, Val Acc: 0.8220\n",
      "Epoch [5 of 20]: Train Loss: 0.3254, Train Acc: 0.8787 | Val Loss: 0.6021, Val Acc: 0.7591\n",
      "Epoch [6 of 20]: Train Loss: 0.2996, Train Acc: 0.8898 | Val Loss: 0.3928, Val Acc: 0.8678\n",
      "Epoch [7 of 20]: Train Loss: 0.2631, Train Acc: 0.9019 | Val Loss: 0.5207, Val Acc: 0.8129\n",
      "Epoch [8 of 20]: Train Loss: 0.2471, Train Acc: 0.9092 | Val Loss: 0.3123, Val Acc: 0.8836\n",
      "Epoch [9 of 20]: Train Loss: 0.2223, Train Acc: 0.9169 | Val Loss: 0.2898, Val Acc: 0.8993\n",
      "Epoch [10 of 20]: Train Loss: 0.2100, Train Acc: 0.9231 | Val Loss: 0.2685, Val Acc: 0.8991\n",
      "Epoch [11 of 20]: Train Loss: 0.1905, Train Acc: 0.9282 | Val Loss: 0.4114, Val Acc: 0.8524\n",
      "Epoch [12 of 20]: Train Loss: 0.1728, Train Acc: 0.9376 | Val Loss: 0.2738, Val Acc: 0.8984\n",
      "Epoch [13 of 20]: Train Loss: 0.1604, Train Acc: 0.9405 | Val Loss: 0.2483, Val Acc: 0.9091\n",
      "Epoch [14 of 20]: Train Loss: 0.1252, Train Acc: 0.9536 | Val Loss: 0.1981, Val Acc: 0.9320\n",
      "Epoch [15 of 20]: Train Loss: 0.1027, Train Acc: 0.9617 | Val Loss: 0.4108, Val Acc: 0.8629\n",
      "Epoch [16 of 20]: Train Loss: 0.0963, Train Acc: 0.9646 | Val Loss: 0.2695, Val Acc: 0.9162\n",
      "Epoch [17 of 20]: Train Loss: 0.0561, Train Acc: 0.9797 | Val Loss: 0.2322, Val Acc: 0.9271\n",
      "Epoch [18 of 20]: Train Loss: 0.0359, Train Acc: 0.9882 | Val Loss: 0.2196, Val Acc: 0.9369\n",
      "Epoch [19 of 20]: Train Loss: 0.0203, Train Acc: 0.9940 | Val Loss: 0.2243, Val Acc: 0.9384\n",
      "Early stopping\n",
      "Test Loss: 0.2050, Test Accuracy: 0.9396\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "def experiment_for_resnext_improved(num_classes, optimizer_name='SGD', batch_size=32, num_epochs=20):\n",
    "    print(f\"ResNeXT = {optimizer_name}, batch_size = {batch_size} ---\")\n",
    "    model = ResNeXt(num_classes=num_classes, cardinality=64, base_width=4, dropout_val=0.1).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    model, history = train_model(model, train_loader, val_loader, scheduler=scheduler,\n",
    "                                 num_epochs=num_epochs, patience=5, optim=optimizer,\n",
    "                                 optim_str=optimizer_name, batch_size=batch_size)\n",
    "    test_loss, test_acc = validate(model, test_loader, nn.CrossEntropyLoss())\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "model_resnext_improved, history_resnext_improved = experiment_for_resnext_improved(num_classes=num_classes, optimizer_name='SGD', batch_size=64, num_epochs=20)\n",
    "torch.save(model_resnext_improved.state_dict(), \"resnext_improved_best_weights.pth\")\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FafZ9bj-G81r"
   },
   "source": [
    "## Step 3: Compare the performance of your ResNeXt model\n",
    "Compare the performance of your ResNeXt model against your previous ResNet and VGG models. Provide a detailed analysis of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmVvqLP5G81s"
   },
   "source": [
    "### a. A table summarizing the performance metrics (accuracy, loss, etc.) for all three models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10xbS-ayG81s"
   },
   "source": [
    "<table border=\"1\" cellspacing=\"0\" cellpadding=\"5\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Model</th>\n",
    "      <th>Validation Loss</th>\n",
    "      <th>Validation Accuracy</th>\n",
    "      <th>Test Loss</th>\n",
    "      <th>Test Accuracy</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>ResNet18</td>\n",
    "      <td>0.2326</td>\n",
    "      <td>91.27%</td>\n",
    "      <td>0.2194</td>\n",
    "      <td>91.64%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>ResNeXt</td>\n",
    "      <td>0.2243</td>\n",
    "      <td>93.84%</td>\n",
    "      <td>0.2050</td>\n",
    "      <td>93.96%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>VGG16-C</td>\n",
    "      <td>0.2538</td>\n",
    "      <td>90.78%</td>\n",
    "      <td>0.2348</td>\n",
    "      <td>91.40%</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuYWnPXwG81s"
   },
   "source": [
    "### b. Discussion of the observed differences in performance.\n",
    "Explain why ResNeXt might be outperforming ResNet and VGG. Consider factors like cardinality, grouped convolutions, and the overall architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7N78AL2-G81s"
   },
   "source": [
    "ResNeXt outperforms ResNet and VGG primarily because it introduces a new aspect called cardinality to the network structure. Cardinality is utilized to refer to the number of parallel transformation path ways per block. Instead of having deeper or wider layers stacked on top of each other, ResNeXt adds a group of transformations in parallel. Through this additions, the network can learn more rich features without a proportional increase in parameters, which leads to better utilization of model capacity.\n",
    "\n",
    "Another significant advantage is the use of grouped convolutions. In ResNeXts approach, the second convolution is replaced by a grouped convolutions where the input channels are split into several groups and convolution operations are done independently on each group. Not only does this save computational cost, but it also forces the network to learn distinct feature representations in each group. The group output then maintains a high proportion of features such that the network is able to generalize better on complex visual tasks.\n",
    "\n",
    "We could see that the number of parameters used by resnext is way lesser compared to VCG AND RESNET , less parameters but more accuracy .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JL-01KS7G81s"
   },
   "source": [
    "\n",
    "### c. Analysis of any challenges encountered during the implementation or training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXM_48owG81s"
   },
   "source": [
    "There were some difficulties we faced during development. Hyperparameter tuning was sensitive slight variations in dropout or learning rate would have a significant effect on performance, so achieving the correct balance took a bit of trial and error. The increased complexity of the model with the grouped convolutions and multiple branches also made it more difficult to keep track of dimensions and make sure everything functioned correctly compared to more straightforward architectures. We also saw some volatility in validation accuracy, which had to be carefully watched and countered with techniques like early stopping to avoid overfitting. Overall, although these problems slowed us down a bit, they provided us with greater insight into how to bring out the best in the ResNeXt architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoM3Y1kbG81s"
   },
   "source": [
    "### d. Provide detailed analysis of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YP5KAOKmG81t"
   },
   "source": [
    "Training logs indicate consistent decrease in the training loss while the training accuracy increases similarly in the 20 epochs, hitting a high of 99.4% for epoch 19. The validation accuracy continues improving steadily to the highest of about 93.8% among the later epochs while validation loss settles at an average of around 0.22, demonstrating that the model is picking solid features without fitting. Though there are some minor oscillations, a negligible dip in validation accuracy at epochs 5 and 15â€”the overall trend is decisively positive, with test accuracy of about 93.96% on the final model and minimal test loss of 0.2050. The early stopping condition appears to have been successful at training when performance converged, which indicates that the best ResNeXt architecture with the hyperparameters optimized is efficient and generalizes well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clyZwAMwG81t"
   },
   "source": [
    "### 4.\tReferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rg3qurlvG81t"
   },
   "source": [
    "https://arxiv.org/abs/1611.05431 -- Resnext\n",
    "\n",
    "Assignment1 part 1\n",
    "\n",
    "Assignment1 part 2\n",
    "\n",
    "https://youtu.be/l7CK-u8InsA?si=jNtLY8Tkaj8DRWLZ - resnext tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
